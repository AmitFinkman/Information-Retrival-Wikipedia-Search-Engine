{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac36d3a",
   "metadata": {
    "id": "5ac36d3a",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-Worker_Count",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "cf88b954-f39a-412a-d87e-660833e735b6"
   },
   "outputs": [],
   "source": [
    "# if the following command generates an error, you probably didn't enable \n",
    "# the cluster security option \"Allow API access to all Google Cloud services\"\n",
    "# under Manage Security â†’ Project Access when setting up the cluster\n",
    "!gcloud dataproc clusters list --region us-central1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cf86c5",
   "metadata": {
    "id": "51cf86c5"
   },
   "source": [
    "# Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bf199e6a",
   "metadata": {
    "id": "bf199e6a",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-Setup",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "fc0e315d-21e9-411d-d69c-5b97e4e5d629"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip install -q google-cloud-storage==1.43.0\n",
    "!pip install -q graphframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f56ecd",
   "metadata": {
    "id": "d8f56ecd",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-Imports",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "a24aa24b-aa75-4823-83ca-1d7deef0f0de"
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "import sys\n",
    "from collections import Counter, OrderedDict, defaultdict\n",
    "import itertools\n",
    "from itertools import islice, count, groupby\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from operator import itemgetter\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "import builtins\n",
    "\n",
    "\n",
    "import hashlib\n",
    "def _hash(s):\n",
    "    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3be10b4-139d-44c2-a56c-57f94848f79e",
   "metadata": {},
   "source": [
    "### runpyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d385a5-20d7-4119-8325-0180b3403d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf, SparkFiles\n",
    "from pyspark.sql import SQLContext\n",
    "from graphframes import *\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9607bb-2902-4c3a-a060-991f8f4df0c0",
   "metadata": {},
   "source": [
    "### Check if invered index.py in dataproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a01ece-dd5a-406e-aa6a-0fbbe80d24dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if nothing prints here you forgot to upload the file inverted_index_gcp.py to the home dir\n",
    "%cd -q /home/dataproc\n",
    "!ls inverted_index_gcp.py\n",
    "# adding our python module to the cluster\n",
    "sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n",
    "sys.path.insert(0,SparkFiles.getRootDirectory())\n",
    "from inverted_index_gcp import InvertedIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2538d527-466c-409a-82d3-c28cb74dce05",
   "metadata": {
    "id": "47900073",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-pyspark-import",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### load data from main bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980e62a5",
   "metadata": {
    "id": "980e62a5",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bucket_name",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "bucket_name = 'main_buckett' \n",
    "full_path = f\"gs://{bucket_name}/\"\n",
    "paths=[]\n",
    "\n",
    "client = storage.Client()\n",
    "blobs = client.list_blobs(bucket_name)\n",
    "for b in blobs:\n",
    "    if b.name != 'graphframes.sh':\n",
    "        paths.append(full_path+b.name)\n",
    "corpus  = spark.read.parquet(*paths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac891c2",
   "metadata": {
    "id": "cac891c2"
   },
   "source": [
    "***GCP setup is complete!*** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582c3f5e",
   "metadata": {
    "id": "582c3f5e"
   },
   "source": [
    "# Building an inverted index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481f2044",
   "metadata": {
    "id": "481f2044"
   },
   "source": [
    "Here, we read the entire corpus to an rdd, directly from Google Storage Bucket The number of pages should be more than 6M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c523e7",
   "metadata": {
    "id": "e4c523e7",
    "outputId": "988d6a6c-4383-41d7-9141-f0e04246dfc8"
   },
   "outputs": [],
   "source": [
    "corpus  = spark.read.parquet(*paths)\n",
    "# Count number of wiki pages\n",
    "corpus.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5540c727",
   "metadata": {
    "id": "5540c727"
   },
   "source": [
    "## Build inverted index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a928bbe2-259c-4c11-8573-2031c66e6f89",
   "metadata": {},
   "source": [
    "### fucntion to use -> from HW3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f3ad8fea",
   "metadata": {
    "id": "f3ad8fea",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-token2bucket",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "english_stopwords = frozenset(stopwords.words('english'))\n",
    "#we can add more stop words!!\n",
    "corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n",
    "                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\", \n",
    "                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n",
    "                    \"many\", \"however\", \"would\", \"became\"]\n",
    "\n",
    "all_stopwords = english_stopwords.union(corpus_stopwords)\n",
    "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n",
    "\n",
    "def token2bucket_id(token,NUM_BUCKETS = 124):\n",
    "    return int(_hash(token),16) % NUM_BUCKETS\n",
    "\n",
    "\n",
    "def tokenizer(text, bigram=False,stemmer = True, trigram=False):\n",
    "    \"\"\"\n",
    "    this funciton do tokanize to text -> it can be use with stem/or ngrams(2 or 3)\n",
    "    \"\"\"\n",
    "    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n",
    "    if stemmer:\n",
    "        tokens = [stemmer.stem(term) for term in tokens if term not in all_stopwords]\n",
    "    if not ngram: \n",
    "        return tokens\n",
    "    else:\n",
    "        if len(tokens)<2: return []\n",
    "        ngrams_tokens = [token[0] + " " + token[1] for token in list(nltk.bigrams(tokens))]\n",
    "        return ngrams_tokens\n",
    "        \n",
    "def word_count(text, id, ngram=False,stem = True):\n",
    "    \"\"\"\n",
    "    this funciton Count number of apperance of word in text.\n",
    "    \"\"\"\n",
    "    tokenizer = tokenizer(text, ngram,stem)\n",
    "    return [(w,(id,f)) for w,f in Counter(tokens).items()]\n",
    "         \n",
    "def reduce_word_counts(unsorted_pl):\n",
    "    \"\"\"\n",
    "    Sorting posting list by freq of the docs    \n",
    "    \"\"\"\n",
    "    return sorted(unsorted_pl, key = lambda x: x[0], reverse = True)\n",
    "\n",
    "def calculate_df(postings):\n",
    "    \n",
    "    \"\"\" \n",
    "    calculate doc frequency of each posting term   \n",
    "    \"\"\"\n",
    "\n",
    "    return postings.map(lambda x: (x[0], len(x[1])))\n",
    "\n",
    "def partition_postings_and_write(postings,bucket_name, bucket_num):\n",
    "    \"\"\"write posting list to buckets in GCP \"\"\"\n",
    "  \n",
    "    token2bucket_RDD = postings.map(lambda x: (token2bucket_id(x[0],bucket_num), x)).groupByKey() #we can add sort for the bucket mybe\n",
    "    write_RDD = token2bucket_RDD.map(lambda x: InvertedIndex.write_a_posting_list(x,bucket_name))\n",
    "    return write_RDD\n",
    "\n",
    "\n",
    "def calc_tdidf(text, id ,df ,corpus_size, ngram=False):\n",
    "     \"\"\"Calculate the size in the TD-IDF \"\"\"\n",
    "    tokens = tokenizer(text, ngram)\n",
    "    words_counter = Counter(tokens)\n",
    "    size = builtins.sum([(c/len(tokens)*math.log2(corpus_size/df[w]))**2 for w,c in words_counter.items() if w in df])\n",
    "    return (id, math.sqrt(size))\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce38fb3a-bc92-4d61-9e8d-57691b6d395a",
   "metadata": {},
   "source": [
    "### Inverted_Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12775a39-d19f-4f96-8873-f0b8b3285268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_Inverted_Index(corpus, bucket_name, column, NUM_BUCKETS, filter_num=50,bgram=False,stem = True):\n",
    "    # time the index creation time\n",
    "    t_start = time()\n",
    "    \n",
    "    doc_pairs = corpus.select(column, \"id\").rdd \n",
    "    word_counts = doc_text_pairs.flatMap(lambda x: word_count(x[0], x[1],bgram,stem))\n",
    "    \n",
    "    postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n",
    "    \n",
    "    # filtering postings \n",
    "    postings_filtered = postings.filter(lambda x: len(x[1])>filter_num) # maybe add filter to the upper bound\n",
    "    \n",
    "    #caclculate term freq in corpus\n",
    "    term_total = postings_filtered.map(lambda x: (x[0],builtins.sum([i[1] for i in x[1]])))\n",
    "    \n",
    "     #caclculate each doc leangh\n",
    "    doc_len = doc_pairs.map(lambda x: (x[1], len(get_tokens(x[0]))))\n",
    "    \n",
    "    \n",
    "    w2df = calculate_df(postings_filtered)\n",
    "    \n",
    "    _ = partition_postings_and_write(postings_filtered, bucket_name, NUM_BUCKETS).collect()\n",
    "    \n",
    "    # collect all posting lists locations into one super-set\n",
    "    super_posting_locs = defaultdict(list)\n",
    "    for blob in client.list_blobs(bucket_name, prefix='postings_gcp'):\n",
    "        if not blob.name.endswith(\"pickle\"):\n",
    "            continue\n",
    "        with blob.open(\"rb\") as f:\n",
    "            posting_locs = pickle.load(f)\n",
    "            for k, v in posting_locs.items():\n",
    "                super_posting_locs[k].extend(v)\n",
    "\n",
    "                \n",
    "    # Create inverted index instance\n",
    "    inverted = InvertedIndex(bucket_name)\n",
    "    # Adding the posting locations dictionary to the inverted index\n",
    "    inverted.posting_locs = super_posting_locs\n",
    "    # Add the token - df dictionary to the inverted index\n",
    "    inverted.df = w2df.collectAsMap()\n",
    "    # Add the Document leangth -> use for bm25\n",
    "    inverted.DL = doc_len.collectAsMap()\n",
    "    # Add the term_total\n",
    "    inverted.term_total = term_total.collectAsMap()\n",
    "    #calculate tdidf\n",
    "    tf_score = doc_pairs.map(lambda x: calc_tdidf(x[0], x[1], w2df_dict, len(inverted.DL), ngram))\n",
    "    inverted.DS = dict(tf_score.collect())\n",
    "    # write the global stats out\n",
    "    inverted.write_index('.', f'index_{bucket_name}')\n",
    "    \n",
    "    # upload to gs\n",
    "    index_src = f\"index_{bucket_name}.pkl\"\n",
    "    index_dst = f'gs://{bucket_name}/postings_gcp/{index_src}'\n",
    "    !gsutil cp $index_src $index_dst\n",
    "    index_const_time = time() - t_start\n",
    "    print(f\"index_time = \"index_const_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23fa014-a96b-467e-b4d6-f94213fd09d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#arguments\n",
    "buckets = [\"title_bucket0\",'title_bucket_ngram',\"title_bucket_nostem\",'body_bucket0','body_bucket_ngram',\"body_bucket_nostem\"]\n",
    "columns = [\"title\",\"title\",\"title\",\"text\",\"text\",\"text\"]\n",
    "sizes = [124,124,124,248,124,124]\n",
    "filter_size = [0,0,0,50,50,50]\n",
    "bgrams =  [False,True,False,False,True,False]\n",
    "steming = [True,True,False,True,True,True]\n",
    "\n",
    "#create all inveted indexs -> take long time\n",
    "for indx in range(len(buckets)):\n",
    "    Create_Inverted_Index(corpus, buckets[indx], columns[indx], sizes[indx], filter_size[indx],bgrams[indx],steming[indx])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52dee14",
   "metadata": {
    "id": "c52dee14",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2a6d655c112e79c5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# PageRank "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0875c6bd",
   "metadata": {
    "id": "0875c6bd",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2fee4bc8d83c1e2a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Compute PageRank for the entire English Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a516e2",
   "metadata": {
    "id": "31a516e2"
   },
   "outputs": [],
   "source": [
    "def generate_graph(pages):\n",
    "    edges = pages.flatMapValues(lambda x: x).map(lambda x: Row(x[0], x[1][0])).distinct()\n",
    "    vertices = edges.flatMap(lambda x: x).distinct().map(lambda x: Row(x))\n",
    "    return edges, vertices\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc05ba3",
   "metadata": {
    "id": "6bc05ba3",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-PageRank",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "688b6f87-6790-4574-bfbd-ac1d8563bbce"
   },
   "outputs": [],
   "source": [
    "t_start = time()\n",
    "pages_links = spark.read.parquet(\"gs://wikidata_preprocessed/*\").select(\"id\", \"anchor_text\").rdd\n",
    "# construct the graph \n",
    "edges, vertices = generate_graph(pages_links)\n",
    "# compute PageRank\n",
    "edgesDF = edges.toDF(['src', 'dst']).repartition(124, 'src')\n",
    "verticesDF = vertices.toDF(['id']).repartition(124, 'id')\n",
    "g = GraphFrame(verticesDF, edgesDF)\n",
    "pr_results = g.pageRank(resetProbability=0.15, maxIter=6)\n",
    "pr = pr_results.vertices.select(\"id\", \"pagerank\")\n",
    "pr = pr.sort(col('pagerank').desc())\n",
    "pr.repartition(1).write.csv('gs://title_bucket0/pr', compression=\"gzip\")\n",
    "pr_time = time() - t_start\n",
    "pr.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386965e2-e9e7-4678-b707-644bbb5b85a9",
   "metadata": {},
   "source": [
    "# Anchor inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d0c301-8086-471a-98da-f3d65fe57f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'anchor_bucket0'\n",
    "anchor_InvertedIndex = InvertedIndex()\n",
    "links = corpus.select(\"anchor_text\").rdd\n",
    "\n",
    "word_counts = links.flatMap(lambda x:[(get_tokens(text), id) for id, text in x[0]]) #list of token and their id\n",
    "\n",
    "postings = word_counts.flatMap(lambda x: [(y,x[1]) for  y in x[0]]).groupByKey()\n",
    "anchor_posting = postings.mapValues(list).map(lambda x: (x[0], Counter(x[1]).most_common()))\n",
    "\n",
    "postings_filtered = anchor_posting.filter(lambda x: len(x[1]) > 0) # (\"word\",{doc_id1:number of appearances})\n",
    "\n",
    "anchor_InvertedIndex.df = Counter(calculate_df(postings_filtered).collectAsMap())\n",
    "posting_locs_list = partition_postings_and_write(postings_filtered, bucket_name, 124).collect()\n",
    "\n",
    "\n",
    "\n",
    "# collect all posting lists locations into one super-set\n",
    "super_posting_locs_text = defaultdict(list)\n",
    "for blob in client.list_blobs(bucket_name, prefix='postings_gcp'):\n",
    "  if not blob.name.endswith(\"pickle\"):\n",
    "    continue\n",
    "  with blob.open(\"rb\") as f:\n",
    "    posting_locs = pickle.load(f)\n",
    "    for k, v in posting_locs.items():\n",
    "      super_posting_locs_text[k].extend(v)\n",
    "anchor_InvertedIndex.posting_locs = super_posting_locs_text\n",
    "# upload to gs\n",
    "anchor_InvertedIndex.write_index('.', f'index_{bucket_name}')\n",
    "index_src = f\"index_{bucket_name}.pkl\"\n",
    "index_dst = f'gs://{bucket_name}/postings_gcp/{index_src}'\n",
    "!gsutil cp $index_src $index_dst"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "ff4b1fca65a764b45acb559e482afe389d289dd599b9f8c5fd12ff5c2ea46a65"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
